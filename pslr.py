# -*- coding: utf-8 -*-
"""PSLR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N5CBgNlQvPwdEhVUgMNctIK3t9xe6x6z
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/faisal_exp

import numpy as np
from matplotlib import pyplot as plt

import pandas as pd
import numpy as np
X = pd.read_excel (r'/content/drive/MyDrive/faisal_exp/nir_specto.xlsx',header=None)
X = X.iloc[:,1:580] 
X=X[1:58]
X=np.array(X)
print(X.shape)

import pandas as pd
Y_p = pd.read_excel (r'/content/drive/MyDrive/faisal_exp/lab(%).xlsx')
Y_p = Y_p.iloc[:,1:8]
#Y_p = Y_p.iloc[:,2]
Y_p=np.array(Y_p)
print (Y_p.shape)

import pandas as pd
Y_c = pd.read_excel (r'/content/drive/MyDrive/faisal_exp/lab(content).xlsx')
Y_c = Y_c.iloc[:,2:9]
#Y_c = Y_c.iloc[:,2]
Y_c=np.array(Y_c)
print (Y_c.shape)

#Expand dataset, Function also available in ChemUtils
def dataaugment(x, betashift = 0.05, slopeshift = 0.05,multishift = 0.05):
    #Shift of baseline
    #calculate arrays
    beta = np.random.random(size=(x.shape[0],1))*2*betashift-betashift
    slope = np.random.random(size=(x.shape[0],1))*2*slopeshift-slopeshift + 1
    #Calculate relative position
    axis = np.array(range(x.shape[1]))/float(x.shape[1])
    #Calculate offset to be added
    offset = slope*(axis) + beta - axis - slope/2. + 0.5

    #Multiplicative
    multi = np.random.random(size=(x.shape[0],1))*2*multishift-multishift + 1

    x = multi*x + offset

    return x

np.random.seed(0)

#Data Augment a single spectrum
import numpy as np
from matplotlib import pyplot as plt
#First Spectrum
X_one = X[0:1]
#Repeating the spectrum 10x
X_one = np.repeat(X_one, repeats=10, axis=0)
#Augment (Large pertubations for illustration)
X_aug = dataaugment(X_one,betashift = 6.542472601263714e-05, slopeshift = 0.0005,multishift = 6.542472601263714e-05)
    
plt.plot(X_aug.T)
_= plt.plot(X_one.T, lw=1, c='b')

shift = np.std(X)*0.1
shift

X_train_aug = np.repeat(X, repeats=3, axis=0)
X_train_aug = dataaugment(X_train_aug, betashift = shift, slopeshift = 0.0005, multishift = shift)
# = s = np.random.normal(Y_mean, Y_std, 171)
y_train_aug = np.repeat(Y_c, repeats=3, axis=0) #y_train is simply repeated
#X_test_aug = np.repeat(X_test, repeats=10, axis=0)
#X_test_aug = dataaugment(X_test_aug, betashift = shift, slopeshift = 0.05, multishift = shift)

#y_test_aug = np.repeat(y_test, repeats=10, axis=0) #y_train is simply repeated


print (len(X_train_aug))
print (len(y_train_aug))
_ = plt.plot(X_train_aug[0:100].T)

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_train_aug, y_train_aug, test_size=0.30, random_state=42)
 
# Quick sanity check with the shapes of Training and testing datasets
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# data normalization with sklearn
from sklearn.preprocessing import MinMaxScaler

# fit scaler on training data
norm = MinMaxScaler().fit(X_train)
# fit scaler on training data
norm_lab = MinMaxScaler().fit(y_train)

# transform training data
X_train_norm = norm.transform(X_train)

# transform testing dataabs
X_test_norm = norm.transform(X_test)
# transform training label
Y_train_norm = norm_lab.transform(y_train)

# transform testing label
Y_test_norm = norm_lab.transform(y_test)

#Rescale to NN friendly number range
from ChemUtils import GlobalStandardScaler

xscaler = GlobalStandardScaler()

#Calibrate is smaler than test, so they are swapped
X_train = xscaler.fit_transform(X_train) #From instrument 1
X_test=xscaler.transform(X_test)

yscaler = GlobalStandardScaler()
y_train = yscaler.fit_transform( y_train)
y_test = yscaler.transform( y_test)

from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=60)
pls2.fit(X_train_norm, Y_train_norm)

from sklearn.metrics import r2_score
print(r2_score(Y_test_norm, pls2.predict(X_test_norm), multioutput='raw_values'))
print(r2_score(Y_train_norm, pls2.predict(X_train_norm),multioutput='raw_values'))

y_train=norm_lab.inverse_transform(Y_train_norm)
y_test=norm_lab.inverse_transform(Y_test_norm)
y_pred1_train=pls2.predict(X_train_norm)
y_pred_train=norm_lab.inverse_transform(y_pred1_train)
y_pred1_test=pls2.predict(X_test_norm)
y_pred_test=norm_lab.inverse_transform(y_pred1_test)

from sklearn.metrics import mean_squared_error
print(np.sqrt(mean_squared_error(y_test,y_pred_test ,multioutput='raw_values')))

rmse_t=np.sqrt(mean_squared_error(y_train,y_pred_train ,multioutput='raw_values')) 
res = rmse_t.astype(np.float)
print(res)

j=y_pred_test[:,0]
j

fig = plt.figure(figsize=(6, 4))
plt.xlabel("Protein measured (mg/100 mg)") 
plt.ylabel("Protein predicted (mg/100 mg)") 
plt.scatter(y_train[:,0],y_pred_train[:,0])
plt.scatter(y_test[:,0], y_pred_test[:,0])
plt.legend(["Training samples", "Testing samples"], loc ="upper left") 
plt.plot([15,40],[15,40])  
plt.savefig('protein.pdf',bbox_inches='tight')

#Some metrics
def huber(y_true, y_pred, delta=1.0):
	y_true = y_true.reshape(-1,1)
	y_pred = y_pred.reshape(-1,1)
	return np.mean(delta**2*( (1+((y_true-y_pred)/delta)**2)**0.5 -1))

def benchmark(X_train,y_train,X_test, y_test, model):
    rmse = np.mean((y_train - pls2.predict(X_train).reshape(y_train.shape))**2)**0.5
    rmse_test = np.mean((y_test - pls2.predict(X_test).reshape(y_test.shape))**2)**0.5
    hub = huber(y_train, pls2.predict(X_train))
    hub_test = huber(y_test, pls2.predict(X_test))
    print ("RMSE  Train/Test\t%0.2F\t%0.2F"%(rmse, rmse_test))
    print ("Huber Train/Test\t%0.4F\t%0.4F"%(hub, hub_test))

benchmark(X_train, y_train, X_test, y_test, pls2)